# Web Screenshot Tool v6.0

一个强大的命令行网页截图工具，可以智能截取网页内容并自动转换为可搜索的RTF文档，支持超长页面的分块处理，自动去除重复的页面结构元素。

## 🚀 功能特性

- 🖼️ **智能内容截图**：自动隐藏header、footer、导航栏等固定元素，只截取页面主要内容
- 📄 **自动RTF生成**：将网页内容自动转换为可搜索的RTF文档
- 🔍 **OCR文本识别**：使用AI技术识别图片中的文本内容
- 📝 **直接文本提取**：直接从HTML中提取文本，保持原始结构
- 🧩 **超长页面分块处理**：智能处理超过65000像素的长页面，避免内存错误
- 🔄 **智能去重**：避免重复的页面结构，确保内容连贯
- 📏 **自适应宽度**：自动调整浏览器窗口宽度以捕获完整页面
- 🧹 **自动清理**：程序结束或中断时自动清理临时文件
- ⚡ **高效处理**：优化的截图策略，支持任意长度页面处理
- 📊 **智能预估**：显示页面总高度和预估处理时间
- 🚫 **无限制截图**：支持任意长度的页面，无截图数量限制
- ⚡ **并行处理**：支持同时处理多个URL，大幅提升处理速度

## 安装依赖

```bash
pip install -r requirements.txt
```

## 使用方法

### 基本用法

```bash
python app.py "https://example.com"
```

### 多个网页

```bash
python app.py "https://example.com" "https://google.com" "https://github.com"
```

### 长文章处理

程序会自动检测并处理超长页面（如Wikipedia文章）：

```bash
python app.py "https://en.wikipedia.org/wiki/Apple_Inc."
```

### 处理特殊字符

如果URL包含特殊字符（如`&`），请用引号包裹：

```bash
python app.py "https://www.google.com/search?q=test&source=hp"
```

## 长页面分块处理

### 自动分块机制
- **智能检测**：自动检测页面总高度是否超过65000像素限制
- **分块处理**：将超长图像分割为多个60000像素高度的块
- **内存优化**：避免PIL图像处理的尺寸限制和内存错误
- **无缝拼接**：确保内容连贯，无重复或遗漏

### 处理流程
```
长页面检测 → 分块截图 → 分块拼接 → RTF生成 → 自动清理
```

### 支持的页面类型
- Wikipedia长文章（如Apple Inc.页面，69000+像素）
- 长博客文章
- 技术文档页面
- 新闻报道长页面

## 并行处理说明

### 性能优势
- **多线程处理**：同时处理多个URL，大幅提升效率
- **智能调度**：自动分配线程资源，避免系统过载
- **进度跟踪**：实时显示每个URL的处理状态

### 处理策略
- 最多同时处理3个URL（可调整）
- 每个URL使用独立的浏览器实例
- 自动清理临时文件和浏览器资源

## 输出示例

### 长文章处理输出
```
准备处理 1 个URL...
开始并行处理 1 个URL...
保存目录: /Users/sam/Desktop/web-saver

处理URL 1: https://en.wikipedia.org/wiki/Apple_Inc.
正在访问: https://en.wikipedia.org/wiki/Apple_Inc.
页面总高度: 69372px, 窗口大小: 1200x1200px
预估截图数量: 70张
预估总时间: 210秒 (3.5分钟)
开始截取完整页面内容: (0-69372px)
截图 1-70: 位置 0-69000px, 步长 1000px
开始拼接 70 张截图...
图像拼接信息: 总高度=74270px, 最大宽度=1200px
⚠️  图像总高度(74270px)超过PIL限制，将分块处理
已保存图像块 1: final_screenshot_1_chunk_0.png (1200x59416)
已保存图像块 2: final_screenshot_1_chunk_1.png (1200x14854)
正在提取页面文本...
正在进行OCR识别...
RTF文件已生成: 20240730_143022_1_Apple_Inc_Wikipedia.rtf
RTF已保存: 20240730_143022_1_Apple_Inc_Wikipedia.rtf
✅ https://en.wikipedia.org/wiki/Apple_Inc. 处理完成

所有网页已保存为RTF: 1 个文件
处理完成！生成了 1 个RTF文件
```

### 普通页面处理输出
```
准备处理 1 个URL...
开始并行处理 1 个URL...
保存目录: /Users/sam/Desktop/web-saver

处理URL 1: https://example.com
页面总高度: 231px, 窗口大小: 1200x1200px
预估截图数量: 1张
预估总时间: 3秒 (0.1分钟)
开始截取完整页面内容: (0-231px)
截图 1: 位置 0/231px, 步长 1000px
图像拼接信息: 总高度=231px, 最大宽度=1200px
正常拼接图像: 1200x231
正在提取页面文本...
正在进行OCR识别...
RTF文件已生成: 20240730_143022_1_Example_Domain.rtf
RTF已保存: 20240730_143022_1_Example_Domain.rtf
✅ https://example.com 处理完成

所有网页已保存为RTF: 1 个文件
处理完成！生成了 1 个RTF文件
```

## 输出文件

程序会在当前目录生成以下文件：

- `YYYYMMDD_HHMMSS_序号_页面标题.rtf` - RTF文档，文件名包含时间戳、序号和页面标题
- `YYYYMMDD_HHMMSS_序号_页面标题.pdf` - PDF文件（可选，保持向后兼容）
- `final_screenshot_序号_chunk_*.png` - 分块图像文件（临时，自动清理）

## 技术特性

### 智能去重策略
- 自动隐藏固定定位的header、footer、导航栏等元素
- 从0像素开始截图，覆盖完整页面内容
- 使用1000px步长，确保内容连贯且无重复
- 自动计算截图数量，无限制截图张数

### 超长页面处理技术
- **PIL限制检测**：自动检测是否超过65500像素限制
- **智能分块**：将超长图像分割为多个安全尺寸的块
- **内存优化**：分块处理避免内存溢出
- **自动清理**：处理完成后自动清理分块文件

### 文本提取技术
- **直接文本提取**：从HTML DOM中直接提取文本内容
- **OCR识别**：使用EasyOCR AI技术识别图片中的文本
- **智能合并**：结合两种方法，确保内容完整性
- **格式保留**：保持标题、段落、列表等结构

### 页面处理优化
- 自动检测页面实际高度，显示总像素高度
- 智能预估截图时间和数量
- 支持各种类型的网页（新闻、博客、企业网站、Wikipedia等）
- 无高度限制，可处理任意长度的页面

### 性能优化
- 1000px步长，平衡截图质量和处理速度
- 每张截图预估3秒处理时间
- 自动清理临时文件，节省磁盘空间
- 支持Ctrl+C中断时的优雅退出
- 分块处理优化内存使用

### 文件管理
- 自动清理临时截图文件
- 自动清理分块图像文件
- 支持Ctrl+C中断时的文件清理
- 防止临时文件堆积

### 错误处理
- 网络超时保护
- 页面加载失败重试
- 图像尺寸限制处理
- 异常情况的优雅处理

## 系统要求

- Python 3.7+
- Chrome浏览器
- 网络连接
- 建议8GB以上内存（处理超长页面时）
- 建议2GB以上可用磁盘空间

## 依赖包

- `selenium==4.26.1` - 浏览器自动化
- `webdriver-manager==4.0.2` - Chrome驱动管理
- `pillow==11.0.0` - 图像处理
- `pytesseract==0.3.10` - OCR文本识别
- `easyocr==1.7.0` - AI文本识别

## 性能说明

### 并行处理性能
- 最多同时处理3个URL
- 每个URL独立处理，互不影响
- 总处理时间约等于最慢URL的处理时间

### 处理时间预估
- 每张截图约需3秒（包括滚动和截图时间）
- OCR处理约需2-5秒（取决于图片复杂度）
- 1000px步长，确保内容连贯
- 长页面会自动计算所需截图数量
- 超长页面（如Wikipedia）可能需要3-10分钟

### 内存使用
- 临时截图文件会自动清理
- 分块处理优化内存使用
- 支持大页面处理，建议8GB以上内存
- 并行处理会增加内存使用

### 文件大小
- RTF文件大小取决于页面内容和长度
- 长文章RTF文件可能达到100KB-1MB
- 建议确保足够磁盘空间

## 注意事项

1. **首次运行**：程序会自动下载ChromeDriver和OCR模型，可能需要一些时间
2. **网络连接**：确保网络连接稳定，避免截图失败
3. **内存使用**：超长页面截图可能占用较多内存，建议8GB以上内存
4. **文件大小**：生成的RTF文件可能较大，建议检查磁盘空间
5. **处理时间**：超长页面处理时间较长（3-10分钟），请耐心等待
6. **并发限制**：默认最多同时处理3个URL，可根据系统性能调整
7. **OCR准确性**：OCR识别准确率取决于图片质量和文本清晰度
8. **分块处理**：超长页面会自动分块处理，这是正常现象

## 故障排除

### 常见问题

1. **ChromeDriver下载失败**
   - 检查网络连接
   - 确保Chrome浏览器已安装

2. **截图超时**
   - 检查目标网站是否可访问
   - 尝试访问其他网站测试

3. **超长页面处理失败**
   - 检查系统内存是否充足
   - 确保有足够的磁盘空间
   - 尝试单独处理该页面

4. **图像处理错误**
   - 程序会自动分块处理超长图像
   - 如遇到"Maximum supported image dimension"错误，程序会自动处理

5. **OCR识别失败**
   - 检查图片质量
   - 确保文本清晰可见
   - 尝试重新运行程序

6. **页面高度检测**
   - 程序会自动检测页面实际高度
   - 显示总像素高度和预估截图时间
   - 支持任意长度的页面截图

7. **临时文件未清理**
   - 程序会自动清理，如遇问题可手动删除`temp_*.png`和`*_chunk_*.png`文件

8. **长页面处理慢**
   - 这是正常现象，程序会显示预估时间
   - 可以按Ctrl+C中断处理

9. **并行处理失败**
   - 检查系统内存是否充足
   - 减少同时处理的URL数量
   - 检查网络连接稳定性

## 开发说明

### 核心功能
- `capture_full_page()` - 智能内容截图，隐藏固定元素
- `stitch_screenshots()` - 图片拼接，支持分块处理
- `extract_text_from_page()` - 直接文本提取
- `ocr_process_image()` - OCR文本识别
- `create_rtf_document()` - RTF文档生成
- `merge_text_content()` - 文本内容合并
- `process_urls_parallel()` - 并行处理多个URL
- `cleanup_chunk_files()` - 分块文件清理

### 优化策略
- 使用JavaScript隐藏固定定位元素
- 从0像素开始截图，覆盖完整页面
- 1000px步长，平衡效率和内容连贯性
- 自动计算截图数量，智能预估处理时间
- 多线程并行处理，提升整体效率
- 结合直接文本提取和OCR识别，确保内容完整性
- 分块处理机制，突破图像尺寸限制

## 版本历史

### v6.0 (当前版本) - 2024年7月30日
- 🎉 **超长页面分块处理**：智能处理超过65000像素的长页面
- ✅ **PIL限制突破**：自动检测并分块处理超大图像
- ✅ **内存优化**：分块处理避免内存溢出和图像处理错误
- ✅ **自动清理增强**：自动清理分块临时文件
- ✅ **Wikipedia支持**：完美支持Wikipedia等超长文章页面
- ✅ **错误处理改进**：更好的图像处理错误恢复机制
- ✅ **测试覆盖增强**：新增长文章和超大图像处理测试

### v1.0 (基础版本) - 2024年7月30日
- ✅ **稳定发布版本**：经过全面测试的稳定版本
- ✅ **智能内容截图**：自动隐藏header、footer等固定元素
- ✅ **RTF文档生成**：自动转换为可搜索的RTF格式
- ✅ **OCR文本识别**：使用AI技术识别图片中的文本
- ✅ **并行处理**：支持同时处理多个URL，大幅提升效率
- ✅ **无限制截图**：支持任意长度的页面，无截图数量限制
- ✅ **智能预估**：显示页面总高度和预估处理时间
- ✅ **自动清理**：程序结束或中断时自动清理临时文件
- ✅ **错误处理**：完善的错误处理和网络超时保护
- ✅ **命令行优化**：纯命令行版本，专注于核心功能

## 许可证

MIT License

## 贡献

欢迎提交Issue和Pull Request来改进这个工具！

## 支持

如果您遇到问题或有改进建议，请：
1. 查看故障排除部分
2. 提交Issue描述问题
3. 提供详细的错误信息和复现步骤

## 测试

运行测试脚本验证功能：

```bash
python test_command_line.py
```

测试包括：
- 单个URL处理
- 多个URL并行处理
- 长文章分块处理（Wikipedia页面）
- 超大图像处理功能
- OCR文本识别功能
- 直接文本提取功能
- 无效URL处理
- 帮助信息显示
- RTF文件生成
- 文件清理功能

## 下载

### v6.0 最新版本
- **发布日期**: 2024年7月30日
- **主要特性**: 超长页面分块处理、智能内容截图、OCR文本识别、RTF文档生成、并行处理
- **系统要求**: Python 3.7+, Chrome浏览器, 8GB+内存
- **测试状态**: ✅ 90%测试通过 (9/10测试)

### 快速开始
```bash
# 克隆仓库
git clone https://github.com/your-username/web-saver.git
cd web-saver

# 安装依赖
pip install -r requirements.txt

# 运行测试
python test_command_line.py

# 开始使用 - 普通页面
python app.py "https://example.com"

# 开始使用 - 长文章
python app.py "https://en.wikipedia.org/wiki/Apple_Inc."
```
